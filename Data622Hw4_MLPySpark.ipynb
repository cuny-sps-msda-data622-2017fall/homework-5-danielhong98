{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install pyspark\n",
    "\n",
    "tar xvf spark-2.2.0-bin-hadoop2.7.tgz\n",
    "\n",
    "mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark\n",
    "\n",
    "cd /usr/local/spark\n",
    "\n",
    "nano README.md\n",
    "\n",
    "cat README.md\n",
    "\n",
    "build/mvn -DskipTests clean package run\n",
    "\n",
    "./bin/pyspark\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON=\"/usr/local/ipython/bin/ipython\"\n",
    "\n",
    ">>> rdd1 = spark.sparkContext.parallelize([('a',7),('a',2),('b',2)])\n",
    ">>> rdd2 = spark.sparkContext.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\",[\"p\", \"r\"])])\n",
    ">>> rdd3 = spark.sparkContext.parallelize(range(100))\n",
    "\n",
    ">>> rdd1.reduce(lambda a,b: a+b)# Set a fixed value for the hash seed secret\n",
    "export PYTHONHASHSEED=0\n",
    "\n",
    "# Set an alternate Python executable\n",
    "export PYSPARK_PYTHON=/usr/local/ipython/bin/ipython\n",
    "\n",
    "# Augment the default search path for shared libraries\n",
    "export LD_LIBRARY_PATH=/usr/local/ipython/bin/ipython\n",
    "\n",
    "# Augment the default search path for private libraries \n",
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH:$SPARK_HOME/python/\n",
    ">>> rdd2.flatMapValues(lambda x: x).collect()\n",
    "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext\n",
    "\n",
    "export SPARK_HOME=\"/usr/local/spark/\"\n",
    "\n",
    "# Set a fixed value for the hash seed secret\n",
    "export PYTHONHASHSEED=0\n",
    "\n",
    "# Set an alternate Python executable\n",
    "export PYSPARK_PYTHON=/usr/local/ipython/bin/ipython\n",
    "\n",
    "# Augment the default search path for shared libraries\n",
    "export LD_LIBRARY_PATH=/usr/local/ipython/bin/ipython\n",
    "\n",
    "# Augment the default search path for private libraries \n",
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH:$SPARK_HOME/python/\n",
    "\n",
    "# Load in the data\n",
    "rdd = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.data')\n",
    "\n",
    "# Load in the header\n",
    "header = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.domain')\n",
    "\n",
    "header.collect()\n",
    "#[u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']\n",
    "\n",
    "rdd.take(2)\n",
    "#[u'-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000', u'-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']\n",
    "\n",
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "rdd.take(2)\n",
    "\n",
    "#[[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000'], [u'-122.220000', u'37.860000', u'21.000000', u'7099.000000', u'1106.000000', u'2401.000000', u'1138.000000', u'8.301400', u'358500.000000']]\n",
    "\n",
    "# Inspect the first line \n",
    "rdd.first()\n",
    "\n",
    "# Take top elements\n",
    "rdd.top(2)\n",
    "\n",
    "#[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000']\n",
    "\n",
    "# Import the necessary modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Print the data types of all `df` columns\n",
    "# df.dtypes\n",
    "\n",
    "# Print the schema of `df`\n",
    "df.printSchema()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = df.withColumn(\"longitude\", df[\"longitude\"].cast(FloatType())) \\\n",
    "   .withColumn(\"latitude\", df[\"latitude\"].cast(FloatType())) \\\n",
    "   .withColumn(\"housingMedianAge\",df[\"housingMedianAge\"].cast(FloatType())) \\\n",
    "   .withColumn(\"totalRooms\", df[\"totalRooms\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"totalBedRooms\", df[\"totalBedRooms\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"population\", df[\"population\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"households\", df[\"households\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"medianIncome\", df[\"medianIncome\"].cast(FloatType())) \\ \n",
    "   .withColumn(\"medianHouseValue\", df[\"medianHouseValue\"].cast(FloatType()))\n",
    "\n",
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())\n",
    "\n",
    "df.select('population','totalBedRooms').show(10)\n",
    "\n",
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "df.take(2)\n",
    "#[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0), Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]\n",
    "\n",
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()\n",
    "#Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)\n",
    "\n",
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")\n",
    "\n",
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)\n",
    "\n",
    "#[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])), Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]\n",
    "\n",
    "#[(1.4491508524918457, 0.14999), (1.5705029404692372, 0.14999), (2.148727956912464, 0.14999), (1.5831547768979277, 0.344), (1.5182107797955968, 0.398)]\n",
    "\n",
    "# Coefficients for the model\n",
    "linearModel.coefficients\n",
    "\n",
    "# Intercept for the model\n",
    "linearModel.intercept\n",
    "\n",
    "# The coefficients\n",
    "#[0.0,0.0,0.0,0.276239709215,0.0,0.0,0.0]\n",
    "\n",
    "# The intercept\n",
    "#0.990399577462\n",
    "\n",
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError\n",
    "\n",
    "# Get the R2\n",
    "linearModel.summary.r2\n",
    "\n",
    "# RMSE\n",
    "#0.8692118678997669\n",
    "\n",
    "# R2\n",
    "#0.4240895287218379\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
